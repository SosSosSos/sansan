import os
import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores.faiss import FAISS
from langchain.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# å®šæ•°
EMBEDDING_MODEL_NAME="intfloat/multilingual-e5-large"
CHAT_MOCEL_NAME = "gpt-4-1106-preview"
USER_NAME = "user"
ASSISTANT_NAME = "assistant"

# ãƒ­ã‚¸ãƒƒã‚¯
# LLMã®ãƒ­ãƒ¼ãƒ‰
@st.cache_resource
def create_llm():
    # llm = ChatOpenAI(model_name="gpt-3.5-turbo-1106", temperature=0) #, max_tokens=4096)
    llm = ChatOpenAI(model_name=CHAT_MOCEL_NAME, temperature=0)
    return llm

# Vectorç”¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
@st.cache_resource
def create_vectorstore():
    embedding = HuggingFaceEmbeddings(model_name = EMBEDDING_MODEL_NAME)
    vectorstore = FAISS.load_local("./vectorstore", embedding)
    return vectorstore


# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
def create_prompt():
    prompt_template_qa = """ã‚ãªãŸã¯å–¶æ¥­ãƒãƒ³ã§ã™ã€‚ç°¡æ½”ã«ã€äº‹å®Ÿã«åŸºã¥ã„ã¦ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
    ã‚‚ã—ä»¥ä¸‹ã®æƒ…å ±ãŒæ¢ã—ã¦ã„ã‚‹æƒ…å ±ã«é–¢é€£ã—ã¦ã„ãªã„å ´åˆã¯ã€ãã®ãƒˆãƒ”ãƒƒã‚¯ã«é–¢ã™ã‚‹è‡ªèº«ã®çŸ¥è­˜ã‚’ç”¨ã„ã¦è³ªå•
    ã«ç­”ãˆã¦ãã ã•ã„ã€‚
    {context}

    è³ªå•: {question}
    å›ç­”ï¼ˆæ—¥æœ¬èªï¼‰:"""

    prompt_qa = PromptTemplate(
            template=prompt_template_qa,
            input_variables=["context", "question"])
    chain_type_kwargs = {"prompt": prompt_qa}
    return chain_type_kwargs

# è³ªå•å¿œç­”ãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ
def create_chain():
    # ãƒªãƒˆãƒ©ã‚¤ãƒãƒ¼ã®ä½œæˆ
    retriever=create_vectorstore().as_retriever(
            search_kwargs={"k": 300}, # Loraãƒ¢ãƒ‡ãƒ«ã¯5~7,gpt3.5-16kã¯40ã€gpt4ã¯400
            search_type="similarity",
            )
    qa_chain = RetrievalQA.from_chain_type(
        llm=create_llm(),
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs=create_prompt(),
        # return_source_documents=True
        )
    return qa_chain

def run_query(query):
    res = create_chain().run(query)
    return res

# ãƒšãƒ¼ã‚¸æç”»
st.set_page_config(
    page_title="ç”£è³‡ã‚³ãƒ³ã‚¿ã‚¯ãƒˆæ¤œç´¢",
    page_icon="ğŸ§Š",
    layout="wide",
    initial_sidebar_state="expanded",
)
# ãƒªãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
st.sidebar.button('Reload')

# ãƒšãƒ¼ã‚¸å†…å®¹
st.write("### ç”£è³‡ã€€ã‚³ãƒ³ã‚¿ã‚¯ãƒˆè¦ç´„ãªã©")
st.write("æ±äº¬æ”¯åº—ã®ï¼™æœˆã®è¨ªå•å†…å®¹ã‚’ï¼’ï¼ï¼æ–‡å­—ã§è¦ç´„ã—ã¦ã€‚ãªã©ã€‚")
st.write("ã‚³ãƒ³ã‚¿ã‚¯ãƒˆæƒ…å ±ã‚’Vectoræƒ…å ±ã«å¤‰æ›å¾Œã€OpenAIã®APIã‚’åˆ©ç”¨ã—ã¦å†…å®¹è¦ç´„ã—ã¦ã„ã¾ã™ã€‚")

# ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’ä¿å­˜ã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’åˆæœŸåŒ–
if "chat_log" not in st.session_state:
    st.session_state.chat_log = []

user_msg = st.chat_input("ã“ã“ã«è³ªå•ã‚’å…¥åŠ›")

if user_msg:
    # ä»¥å‰ã®ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’è¡¨ç¤º
    for chat in st.session_state.chat_log:
        with st.chat_message(chat["name"]):
            st.write(chat["msg"])

    # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    with st.chat_message(USER_NAME):
        st.write(user_msg)

    # Chainå®Ÿè¡Œã€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    try:
        response = run_query(user_msg)
    except Exception as e:
        response = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

    with st.chat_message(ASSISTANT_NAME):
        assistant_msg = ""
        assistant_response_area = st.empty()

        # å›ç­”ã‚’é€æ¬¡è¡¨ç¤º
        assistant_msg += response
        assistant_response_area.write(assistant_msg)

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’è¿½åŠ 
    st.session_state.chat_log.append({"name": USER_NAME, "msg": user_msg})
    st.session_state.chat_log.append({"name": ASSISTANT_NAME, "msg": assistant_msg})
