import os
import streamlit as st
from streamlit_chat import message
# from langchain.embeddings.openai import OpenAIEmbeddings
# from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
# from langchain.chains import ConversationalRetrievalChain
# from langchain.document_loaders import PyPDFLoader
from langchain.vectorstores import FAISS
# import tempfile
from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain.agents import AgentExecutor
# from google.cloud import bigquery
from langchain.agents import create_sql_agent
from langchain.agents.agent_toolkits import SQLDatabaseToolkit
from langchain.sql_database import SQLDatabase
# from sqlalchemy import *
# from sqlalchemy.engine import create_engine
# from sqlalchemy.schema import *
from langchain.agents.agent_types import AgentType
# from langchain_experimental.agents.agent_toolkits import create_python_agent
# from langchain_experimental.tools import PythonREPLTool

st.set_page_config(
    page_title="ç”£è³‡ã‚³ãƒ³ã‚¿ã‚¯ãƒˆæ¤œç´¢",
    page_icon="ğŸ§Š",
    layout="wide",
    initial_sidebar_state="expanded",
)

st.write("### ç”£è³‡ã€€ã‚³ãƒ³ã‚¿ã‚¯ãƒˆè¦ç´„ãªã©")
st.write("æ±äº¬æ”¯åº—ã®ï¼™æœˆã®è¨ªå•å†…å®¹ã‚’ï¼’ï¼ï¼æ–‡å­—ã§è¦ç´„ã—ã¦ã€‚ãªã©ã€‚")
st.write("ã‚³ãƒ³ã‚¿ã‚¯ãƒˆæƒ…å ±ã‚’Vectoræƒ…å ±ã«å¤‰æ›å¾Œã€OpenAIã®APIã‚’åˆ©ç”¨ã—ã¦å†…å®¹è¦ç´„ã—ã¦ã„ã¾ã™ã€‚")

# os.environ["LANGCHAIN_WANDB_TRACING"] = "true"

@st.cache_resource
def create_llm():
    # llm = ChatOpenAI(model_name="gpt-3.5-turbo-1106", temperature=0) #, max_tokens=4096)
    llm = ChatOpenAI(model_name="gpt-4-1106-preview", temperature=0)
    return llm

# agent_executor = create_python_agent(
#     llm=llm,
#     tool=PythonREPLTool(),
#     verbose=True,
#     agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
# )

# answer = agent_executor.run(
# """
# ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ‡ãƒ¼ã‚¿ã§Dataframeã‚’ä½œã£ã¦
# """
# )

# st.write(answer)



# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ç”¨ã®ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import LlamaCpp
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.schema import Document
from langchain.agents.agent_toolkits import create_retriever_tool
from langchain.memory import ConversationBufferMemory


# VectorIndexã®èª­ã¿è¾¼ã¿
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

hagging_model_name="intfloat/multilingual-e5-large"
embedding = HuggingFaceEmbeddings(model_name = hagging_model_name)

# load
@st.cache_resource
def create_vectorstore():
    vectorstore = FAISS.load_local("./vectorstore", embedding)
    return vectorstore


from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# prompt_template_qa = """ã‚ãªãŸã¯è¦ªåˆ‡ã§å„ªã—ã„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä¸å¯§ã«ã€æ—¥æœ¬èªã§ãŠç­”ãˆãã ã•ã„ï¼
# ã‚‚ã—ä»¥ä¸‹ã®æƒ…å ±ãŒæ¢ã—ã¦ã„ã‚‹æƒ…å ±ã«é–¢é€£ã—ã¦ã„ãªã„å ´åˆã¯ã€ãã®ãƒˆãƒ”ãƒƒã‚¯ã«é–¢ã™ã‚‹è‡ªèº«ã®çŸ¥è­˜ã‚’ç”¨ã„ã¦è³ªå•
# ã«ç­”ãˆã¦ãã ã•ã„ã€‚
prompt_template_qa = """ã‚ãªãŸã¯å–¶æ¥­ãƒãƒ³ã§ã™ã€‚ç°¡æ½”ã«ã€äº‹å®Ÿã«åŸºã¥ã„ã¦ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
ã‚‚ã—ä»¥ä¸‹ã®æƒ…å ±ãŒæ¢ã—ã¦ã„ã‚‹æƒ…å ±ã«é–¢é€£ã—ã¦ã„ãªã„å ´åˆã¯ã€ãã®ãƒˆãƒ”ãƒƒã‚¯ã«é–¢ã™ã‚‹è‡ªèº«ã®çŸ¥è­˜ã‚’ç”¨ã„ã¦è³ªå•
ã«ç­”ãˆã¦ãã ã•ã„ã€‚
{context}

è³ªå•: {question}
å›ç­”ï¼ˆæ—¥æœ¬èªï¼‰:"""

prompt_qa = PromptTemplate(
        template=prompt_template_qa,
        input_variables=["context", "question"]
)
chain_type_kwargs = {"prompt": prompt_qa}

retriever=create_vectorstore().as_retriever(
        search_kwargs={"k": 35}, # Loraãƒ¢ãƒ‡ãƒ«ã¯5~7,gpt3.5-16kã¯40ã€gpt4ã¯400
        search_type="similarity",
        )

# è³ªå•å¿œç­”ãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ
qa_chain = RetrievalQA.from_chain_type(
    llm=create_llm(),
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs=chain_type_kwargs,
    # return_source_documents=True
)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒªãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’ã¤ã‘ã‚‹
st.sidebar.button('Reload')

def run_query(query):
    res = qa_chain.run(query)
    return res

# å®šæ•°å®šç¾©
USER_NAME = "user"
ASSISTANT_NAME = "assistant"

# ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’ä¿å­˜ã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’åˆæœŸåŒ–
if "chat_log" not in st.session_state:
    st.session_state.chat_log = []

user_msg = st.chat_input("ã“ã“ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›")

if user_msg:
    # ä»¥å‰ã®ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’è¡¨ç¤º
    for chat in st.session_state.chat_log:
        with st.chat_message(chat["name"]):
            st.write(chat["msg"])

    # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    with st.chat_message(USER_NAME):
        st.write(user_msg)

    # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    response = run_query(user_msg)
    with st.chat_message(ASSISTANT_NAME):
        assistant_msg = ""
        assistant_response_area = st.empty()
        # for chunk in response:
        #     if chunk.choices[0].finish_reason is not None:
        #         break
            # å›ç­”ã‚’é€æ¬¡è¡¨ç¤º
        assistant_msg += response
        assistant_response_area.write(assistant_msg)

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’è¿½åŠ 
    st.session_state.chat_log.append({"name": USER_NAME, "msg": user_msg})
    st.session_state.chat_log.append({"name": ASSISTANT_NAME, "msg": assistant_msg})
